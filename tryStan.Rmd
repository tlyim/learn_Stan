---
title: "try Stan - following JSS articles"
output: html_notebook
---


```{r}

#library(tidyverse)
library(dplyr)
library(magrittr)
library(rstan)
rstan_options(auto_write = TRUE) # avoid recompilation of unchanged Stan programs

# The following can affect distributional computing over a cluster
options(mc.cores = parallel::detectCores())  # 

# The following throws an error in compiling .stan
#Sys.setenv(LOCAL_CPPFLAGS = '-march=native') # For improved execution time but can throw errors for some CPUs

```

# Simulate GP data
```{stan output.var=simu_gauss, eval=T}
data {
  int<lower=1> N;
  real x[N];

  real<lower=0> rho;
  real<lower=0> alpha;
  real<lower=0> sigma;
}

transformed data {
  matrix[N, N] cov =   cov_exp_quad(x, alpha, rho)
                     + diag_matrix(rep_vector(1e-10, N));
  matrix[N, N] L_cov = cholesky_decompose(cov);
}

parameters {}
model {}

generated quantities {
  vector[N] f = multi_normal_cholesky_rng(rep_vector(0, N), L_cov);
  vector[N] y;
  for (n in 1:N)
    y[n] = normal_rng(f[n], sigma);
}
```

# Estimate the simulated model with Stan - misreporting extent generated from a Gaussian Process
  (forecasts specified in generated quantities block)
```{stan output.var=lmExample, eval=T}

// The input data is a vector 'y' of length 'N'.
data {
  int<lower=0> N; // number of observations (for different CEOs/firms)
  int<lower=0> K; // number of coefficents (/predictors)
  vector[N] y; // dependent variable
  matrix[N,K] X; // predictor variables
  int<lower=0,upper=1> M[N];   // M = 1 for the decision to misreport; = 0 if report honestly  

// forecasts  
  int<lower=0> N_new; // number of predictions
  matrix[N_new,K] X_new; // 
  
// GP data
  int<lower=0> G;
  real z[G];     // ie, x of the GP cov matrix
  vector[G] m;  // misreporting extent
}
transformed data {
// GP data
}
parameters {
  vector[K] b; // coefficients of the predictor variables
  real<lower=0> sd_y; // sd of error term
  real<lower=0,upper=1> integrity;     // integrity level of the society affecting the decision to misreport or not
//  vector[N] integrity;     // integrity level of each CEO affecting the decision to misreport or not

// GP parameters
  real<lower=0> rho;
  real<lower=0> alpha;
  real<lower=0> sigma;
}
transformed parameters {
  real ilogodds;     // integrity level of the society affecting the decision to misreport or not
  ilogodds = logit(integrity);   
}
model {
  matrix[G, G] Kcov;
  matrix[G, G] Lcov;

// b ~ cauchy(0, 2.5); // common prior for each b[K]
  y ~ normal(X*b, sd_y);
  M ~ bernoulli_logit(ilogodds); 

  rho ~ gamma(2, 20);
  alpha ~ normal(0, 1);
  sigma ~ normal(0, 1);

  Kcov = cov_exp_quad(z, alpha, rho) + diag_matrix(rep_vector(square(sigma), G));
  Lcov = cholesky_decompose(Kcov);

  m ~ multi_normal_cholesky(rep_vector(0, G), Lcov);   // rep_vector(0, G) is the zero mean assumed
}

generated quantities {
  vector[N_new] y_new;
  for (n in 1:N_new)
    y_new[n] = normal_rng(X_new[n] * b, sd_y);
}

```


# Simulate simple data and estimate with OLS
(the simplicity gives quite accurate estimates)
```{r}

N = 99  #9999

set.seed(555)  # set the seed for random generation to ensure replicability
# X1 <- rep(1, N)
# X2 <- rnorm(N) # simulate explanatory variable data based on randomly generated numbers following the normal dist
# X3 <- rnorm(N) 
X <- matrix(data = c(rep(1, N), rnorm(N), rnorm(N)), nrow = N)

#set actual parameters of the data generation process (DGP)
#a = c(0.4, 0.1, -0.2)
avec <- matrix(data = c(0.4, 0.1, -0.2), nrow = 3)  # define column vector

# Simulate the logodds based on a linear model 
# (Later, this true logit model will be estimated with observed data)
#logodds <- a[1]*x1 + a[2]*x2 + a[3]*x3  
sigma = 0.15
e <- rnorm(N, sd = sigma)
y <- (X %*% avec + e) %>% as.vector()  # Note: must use the parentheses before %>%; o/w, won't work
# turn the output into a vector, rather than a matrix[N,1]

mod_OLS <- lm(y ~ . + 0, data = data.frame(y, X))  # + 0 to remove intercept as X1 is the unit vector for intercept
summary(mod_OLS)

integrity = 0.8
M <- rbinom(N, 1, integrity)


# Simulate GP data
rho_true <- 5.5
alpha_true <- 3
sigma_true <- 2

N_total = 501
z_total <- 20 * (0:(N_total - 1)) / (N_total - 1) - 10

simu_data <- list(rho=rho_true, alpha=alpha_true, sigma=sigma_true,
                  N=N_total, x=z_total)

simu_fit <- stan(model_code = simu_gauss@model_code, data=simu_data, iter=1,
            chains=1, seed=494838, algorithm="Fixed_param")

f_total <- extract(simu_fit)$f[1,]
m_total <- extract(simu_fit)$y[1,]  # misreporting extent = the sample y of from the true realization GP f

true_realization <- data.frame(f_total, z_total)
names(true_realization) <- c("f_total", "z_total")

observed_idx <- c(5*(1:99))  # Get N+1 observed points
z <- z_total[observed_idx]     
m <- m_total[observed_idx]


# Prepare simulated data for estimation using MCMC
N_new = 3
sim_1_data <- list(N = N, 
                 K = 3,
                 y = y, 
                 X = X,
                 M = M,
                 G = N,
                 z = z,
                 m = m,
                 X_new = X[1:N_new,],
                 N_new = N_new
                 )

```

# Fit the debug and full model of complete pooling
```{r}
# Run the debug model to make sure the Stan code complies properly 
fit0_debug <- stan(model_code = lmExample@model_code, data = sim_1_data, iter = 10, chains = 1)

# Run the full model and refer to the debug model to save compilation time 
system.time({
fit1 <- stan(
#  file = "schools.stan",  # Stan program
#  model_code = completepool@model_code,  # either the @model_code of the model definition 
                                    # or the name of a string object containing the model description
  data = sim_1_data,    # named list of data
  fit = fit0_debug,   # to save compilation time if the debug model was run
#  control = list(adapt_delta = 0.95),    # adjust when there're divergent transitions after warmup
#  chains = 1,             # default is 4 Markov chains
#  cores = 8,
  seed = 123,
#  init = init, # where init = list(list(mu=...,sigma=...), list(mu=..., sigma=...), ...) 
                # where length of list = number of chains
  iter = 2000,            # total number of iterations per chain
#  warmup = 1000,          # number of warmup iterations per chain
  refresh = 1000          # = 0 means no progress shown
  )
})

```

# Summary of the posterior sample for phi
```{r}

ss_complete_pool <- extract(fit1);
#print(fit1, c("phi"), probs = c(0.1, 0.5, 0.9))
print(fit1, probs = c(0.1, 0.5, 0.9))

# effective sample size is good 
# (roughly half the number of posterior draws; 
#  by default Stan uses as many iterations to warmup as it does for drawing the sample).

```








#############################################################################################

# Estimate the simulated model with Stan (forecasts specified in generated quantities block)
```{stan output.var=lmExample, eval=T}

data {
  int<lower=0> N; // number of observations (for different CEOs/firms)
  int<lower=0> K; // number of coefficents (/predictors)
  vector[N] y; // dependent variable
  matrix[N,K] X; // predictor variables
  int<lower=0,upper=1> M[N];   // M = 1 for the decision to misreport; = 0 if report honestly  
  
  int<lower=0> N_new; // number of predictions
  matrix[N_new,K] X_new; //  
}
transformed data {
// X1 = 1
}
parameters {
  vector[K] b; // coefficients of the predictor variables
  real<lower=0> sigma; // sd of error term
  real<lower=0,upper=1> integrity;     // integrity level of the society affecting the decision to misreport or not
//  vector[N] integrity;     // integrity level of each CEO affecting the decision to misreport or not
}
transformed parameters {
  real ilogodds;     // integrity level of the society affecting the decision to misreport or not
  ilogodds = logit(integrity);
}
model {
// b ~ cauchy(0, 2.5); // common prior for each b[K]
  y ~ normal(X*b, sigma);
  M ~ bernoulli_logit(ilogodds); // likelihood
  
/*
   CEO's choice of misreporting (explore) vs. truth-telling (exploit) is modelled as discrete choice
     use bernoulli_logit() ?
   The stochastic process (GP? with -ve due to accrual reversal constraint?) of maximally feasible extent of misreporting
     is exogenous to the CEO but the realized value of this in a period affects the achievable reward of misreporting
     GP mean depends on (slowly changing) time-varying firm-specific, industry-specific, and macroeconomic factors ?
   Assume for simplicity that conditional on the decision to misreport, it is always optimal to choose the max extent
     Determinants of explore-exploit include 
       parameters: CEO-specific Integrity, 
       variables: firm-specific ERC, allowable misreporting limit
   Misreported CEO also learns directly the updated ERC for his firm, the incremental effect of misreporting, and
     *most importantly* the likelihood of being caught through the following period (because he knows there's misreporting). 
     Truth-telling would be harder to learn the likelihood of being caught because observing no restatement by other firms
       cannot clearly tell whether there was no misreporting or misreporting remains not caught. 
     Investors are smart enough to condition their ERC response according to the firm characteristics, etc
       For example, in bad economic times, the reward from misreporting becomes relatively higher. 
    Background noise (ie, likelihood or errors as opposed to irregularities) affects the incentive to misreport   
   To begin, assume CEO works for only one firm for his whole life. 
   Keep in mind that the unrestated earnings is a mix of misreported and truth-told earnings
      Also, assume
      - an exogenous fraction of all misreporting will be discovered in the next period 
          (regardless of the misreporting extent in an instance)
      - the fraction follows a stohastic process (again, with the mean slowly changing over time)
   Thus, the count of restatement each period, adjusted by a multiple, is an estimate of accumulated misreporting
     instances in the past     
*/
  
}
generated quantities {
  vector[N_new] y_new;
  for (n in 1:N_new)
    y_new[n] = normal_rng(X_new[n] * b, sigma);
}

```

# Estimate the simulated model with Stan (forecasts specified in parameters+model blocks)
```{stan output.var=lmExample, eval=T}

data {
  int<lower=0> N; // number of observations
  int<lower=0> K; // number of coefficents (/predictors)
  vector[N] y; // dependent variable
  matrix[N,K] X; // predictor variables
  int<lower=0> N_new; // number of predictions
  matrix[N_new,K] X_new; //  
}
transformed data {
// X1 = 1
}
parameters {
  vector[K] b; // coefficients of the predictor variables
  real<lower=0> sigma; // sd of error term
  vector[N_new] y_new;
}
model {
  y ~ normal(X*b, sigma);
  y_new ~ normal(X_new * b, sigma);
}
generated quantities {
//  vector[N_new] y_new;
//  for (n in 1:N_new)
//    y_new[n] = normal_rng(X_new[n] * b, sigma);
}

```

```{stan output.var=simpleExample, eval=T}

data {
int<lower=0> N; // number of trials
int<lower=0, upper=1> y[N]; // success on trial n
}
parameters {
real<lower=0, upper=1> theta; // chance of success
}
model {
theta ~ uniform(0, 1); // prior
for (n in 1:N)
y[n] ~ bernoulli(theta); // likelihood
}


```


